{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, HfArgumentParser\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from vllm import LLM, SamplingParams\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import sys\n",
    "sys.path.append('/scratch/jiarui14/EM-CoT/Online-DPO-R1')\n",
    "import reward_labeling\n",
    "\n",
    "@dataclass\n",
    "class ScriptArguments:\n",
    "    seed: Optional[int] = field(\n",
    "        default=42,\n",
    "        metadata={\"help\": \"Random seed\"}\n",
    "    )\n",
    "    max_length: Optional[int] = field(\n",
    "        default=2048,\n",
    "        metadata={\"help\": \"Max length of newly generated tokens\"}\n",
    "    )\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default='Qwen/Qwen2.5-Math-7B',\n",
    "        metadata={\"help\": \"Model name or path\"}\n",
    "    )\n",
    "    epochs: Optional[int] = field(\n",
    "        default=1,\n",
    "        metadata={\"help\": \"Number of epochs\"}\n",
    "    )\n",
    "    alpha: Optional[float] = field(\n",
    "        default=0.5,\n",
    "        metadata={\"help\": \"Penalty weight alpha\"}\n",
    "    )\n",
    "    beta: Optional[float] = field(\n",
    "        default=2.0,\n",
    "        metadata={\"help\": \"Penalty weight beta\"}\n",
    "    )\n",
    "    lr: Optional[float] = field(\n",
    "        default=0.5,\n",
    "        metadata={\"help\": \"Learning rate\"}\n",
    "    )\n",
    "    start: Optional[int] = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"Start index\"}\n",
    "    )\n",
    "    end: Optional[int] = field(\n",
    "        default=5,\n",
    "        metadata={\"help\": \"End index\"}\n",
    "    )\n",
    "    stage_1_samples: Optional[int] = field(\n",
    "        default=8,\n",
    "        metadata={\"help\": \"Number of samples for stage 1 per example\"}\n",
    "    )\n",
    "    stage_2_samples: Optional[int] = field(\n",
    "        default=8,\n",
    "        metadata={\"help\": \"Number of samples for stage 2 per example\"}\n",
    "    )\n",
    "    local_index: Optional[int] = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"Local index\"}\n",
    "    )\n",
    "\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--seed', type=int, default=42, help='Random seed')\n",
    "# parser.add_argument('--max_length', type=int, default=2028, help='Max length of newly generated tokens')\n",
    "# parser.add_argument('--model_name_or_path', type=str, default='Qwen/Qwen2.5-Math-7B', help='Model name or path')\n",
    "# parser.add_argument('--epochs', type=int, default=1, help='Number of epochs')\n",
    "# parser.add_argument('--alpha', type=float, default=0.5, help='Penalty weight alpha')\n",
    "# parser.add_argument('--beta', type=float, default=2.0, help='Penalty weight beta')\n",
    "# parser.add_argument('--lr', type=float, default=0.5, help='Learning rate')\n",
    "# script_args = parser.parse_args()\n",
    "\n",
    "script_args = ScriptArguments()\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(script_args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "21\n",
      "17\n",
      "19\n",
      "20\n",
      "18\n",
      "16\n",
      "18\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with open(f'data/stage_1_collected_data_{script_args.local_index}.json', 'r') as f:\n",
    "    ds = json.load(f)\n",
    "ds = Dataset.from_list(ds)\n",
    "\n",
    "with open(f'/scratch/jiarui14/EM-CoT/EM-CoT/data/sample_sizes_{script_args.local_index}.json', 'r') as f:\n",
    "    sample_sizes = json.load(f)\n",
    "\n",
    "with open(f'/scratch/jiarui14/EM-CoT/EM-CoT/data/stage_2_allOutputs_{script_args.local_index}.json') as f:\n",
    "    stage_2_allOutputs = json.load(f)\n",
    "\n",
    "stage_2_collected_data = []\n",
    "corrects_2 = []\n",
    "total_samples = 0\n",
    "for i, item in enumerate(ds):\n",
    "    collected_data = {\n",
    "        'problem': item['problem'],\n",
    "        'answer': item['answer'],\n",
    "        'outputs': []\n",
    "    }\n",
    "    problem_corrects = []\n",
    "    for j in range(len(stage_2_outputs[i])):\n",
    "        # correct = reward_labeling.is_equal(outputs[i].outputs[j].text, item['answer'], dataset_name='math500')\n",
    "        # correct = reward_labeling.is_equal(stage_2_outputs[i][j], item['answer'], dataset_name='math500')\n",
    "        correct = utils.check_correct(stage_2_outputs[i][j], item['answer'], i)\n",
    "        if correct:\n",
    "            problem_corrects.append(j)\n",
    "            # collected_data['outputs'].append(outputs[i].outputs[j].text)\n",
    "            collected_data['outputs'].append(stage_2_outputs[i][j])\n",
    "    corrects_2.append(problem_corrects)\n",
    "    stage_2_collected_data.append(collected_data)\n",
    "    total_samples += len(collected_data['outputs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1246/1246 [00:20<00:00, 62.22it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(script_args.model_name_or_path)\n",
    "max_len = -1\n",
    "max_len_id = -1\n",
    "with open('/scratch/jiarui14/EM-CoT/EM-CoT/data/stage_1_collected_data_2.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "for i,item in enumerate(tqdm(data)):\n",
    "    conv = [\n",
    "        {'role': 'system', 'content': 'Please reason step by step, and put your final answer within \\\\boxed{{}}.'},\n",
    "        {'role': 'user', 'content': item['problem'] + f' Let\\'s think step by step and output the final answer within \\\\boxed{{}}'}\n",
    "    ]\n",
    "    for j,output in enumerate(item['outputs']):\n",
    "        conv_chat = tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n",
    "        conv_chat += output\n",
    "        input_ids = tokenizer(conv_chat, return_tensors='pt').input_ids\n",
    "        if input_ids.shape[1] > max_len:\n",
    "            max_len = input_ids.shape[1]\n",
    "            max_len_id = (i,j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9999/9999 [00:00<00:00, 13660.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# get the first 10000 training samples from numina prompt\n",
    "ds = load_dataset('dsrtrain/numia_prompt')['train']\n",
    "ds2 = load_dataset('FlippyDora/raft_train_numia_prompt')['train']\n",
    "\n",
    "problems = []\n",
    "for item in tqdm(ds2):\n",
    "    problem = item['conversations'][0]['content'].split(' Let\\'s think step by step and output the final answer within \\\\boxed{}')[0]\n",
    "    if problem not in problems:\n",
    "        problems.append(problem)\n",
    "\n",
    "qas = {}\n",
    "for item in tqdm(ds):\n",
    "    if item['problem'] not in qas:\n",
    "        qas[item['problem']] = item['reward_model']['ground_truth']\n",
    "\n",
    "new_ds = []\n",
    "for i in range(len(problems)):\n",
    "    new_ds.append({'problem': problems[i], 'answer': qas[problems[i]]})\n",
    "\n",
    "new_ds = Dataset.from_list(new_ds)\n",
    "new_ds.push_to_hub('FlippyDora/raft1_train_numia_prompt_0-10000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "ds = load_dataset('HuggingFaceH4/MATH-500')['test']\n",
    "script_args.end = min(len(ds), script_args.end)\n",
    "ds = ds.select(range(script_args.start, script_args.end))\n",
    "tokenizer = AutoTokenizer.from_pretrained(script_args.model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 02-27 23:15:23 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='Qwen/Qwen2.5-Math-7B', speculative_config=None, tokenizer='Qwen/Qwen2.5-Math-7B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=Qwen/Qwen2.5-Math-7B, use_v2_block_manager=False, enable_prefix_caching=False)\n",
      "INFO 02-27 23:15:24 model_runner.py:720] Starting to load model Qwen/Qwen2.5-Math-7B...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 197.81 MiB is free. Including non-PyTorch memory, this process has 5.31 GiB memory in use. Process 2940433 has 42.00 GiB memory in use. Of the allocated memory 5.00 GiB is allocated by PyTorch, and 15.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# prepare model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_VISIBLE_DEVICES\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 3\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscript_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/llm.py:158\u001b[0m, in \u001b[0;36mLLM.__init__\u001b[0;34m(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, **kwargs)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    136\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThere is no need to pass vision-related arguments anymore.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    137\u001b[0m engine_args \u001b[38;5;241m=\u001b[39m EngineArgs(\n\u001b[1;32m    138\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    139\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    157\u001b[0m )\n\u001b[0;32m--> 158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_engine \u001b[38;5;241m=\u001b[39m \u001b[43mLLMEngine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mUsageContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_counter \u001b[38;5;241m=\u001b[39m Counter()\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py:445\u001b[0m, in \u001b[0;36mLLMEngine.from_engine_args\u001b[0;34m(cls, engine_args, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    443\u001b[0m executor_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_executor_cls(engine_config)\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Create the LLM engine.\u001b[39;00m\n\u001b[0;32m--> 445\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mengine_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecutor_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexecutor_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_log_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m    \u001b[49m\u001b[43musage_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musage_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstat_loggers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstat_loggers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engine\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/engine/llm_engine.py:249\u001b[0m, in \u001b[0;36mLLMEngine.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config_fields \u001b[38;5;241m=\u001b[39m _load_generation_config_dict(\n\u001b[1;32m    244\u001b[0m     model_config)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_processor \u001b[38;5;241m=\u001b[39m INPUT_REGISTRY\u001b[38;5;241m.\u001b[39mcreate_input_processor(\n\u001b[1;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config)\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_executor \u001b[38;5;241m=\u001b[39m \u001b[43mexecutor_class\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspeculative_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspeculative_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_adapter_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39membedding_mode:\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initialize_kv_caches()\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/executor_base.py:47\u001b[0m, in \u001b[0;36mExecutorBase.__init__\u001b[0;34m(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, multimodal_config, speculative_config, prompt_adapter_config)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeculative_config \u001b[38;5;241m=\u001b[39m speculative_config\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt_adapter_config \u001b[38;5;241m=\u001b[39m prompt_adapter_config\n\u001b[0;32m---> 47\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_executor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/executor/gpu_executor.py:36\u001b[0m, in \u001b[0;36mGPUExecutor._init_executor\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_worker()\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdriver_worker\u001b[38;5;241m.\u001b[39minit_device()\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdriver_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker.py:139\u001b[0m, in \u001b[0;36mWorker.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_runner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/model_runner.py:722\u001b[0m, in \u001b[0;36mGPUModelRunnerBase.load_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    720\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting to load model \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_config\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    721\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m CudaMemoryProfiler() \u001b[38;5;28;01mas\u001b[39;00m m:\n\u001b[0;32m--> 722\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m=\u001b[39m m\u001b[38;5;241m.\u001b[39mconsumed_memory\n\u001b[1;32m    732\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model weights took \u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    733\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_memory_usage \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m30\u001b[39m))\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py:21\u001b[0m, in \u001b[0;36mget_model\u001b[0;34m(model_config, load_config, device_config, parallel_config, scheduler_config, lora_config, multimodal_config, cache_config)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_model\u001b[39m(\u001b[38;5;241m*\u001b[39m, model_config: ModelConfig, load_config: LoadConfig,\n\u001b[1;32m     15\u001b[0m               device_config: DeviceConfig, parallel_config: ParallelConfig,\n\u001b[1;32m     16\u001b[0m               scheduler_config: SchedulerConfig,\n\u001b[1;32m     17\u001b[0m               lora_config: Optional[LoRAConfig],\n\u001b[1;32m     18\u001b[0m               multimodal_config: Optional[MultiModalConfig],\n\u001b[1;32m     19\u001b[0m               cache_config: CacheConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m nn\u001b[38;5;241m.\u001b[39mModule:\n\u001b[1;32m     20\u001b[0m     loader \u001b[38;5;241m=\u001b[39m get_model_loader(load_config)\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mdevice_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparallel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:324\u001b[0m, in \u001b[0;36mDefaultModelLoader.load_model\u001b[0;34m(self, model_config, device_config, lora_config, multimodal_config, parallel_config, scheduler_config, cache_config)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_default_torch_dtype(model_config\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m target_device:\n\u001b[0;32m--> 324\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43m_initialize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_weights(\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_weights_iterator(model_config\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[1;32m    329\u001b[0m                                    model_config\u001b[38;5;241m.\u001b[39mrevision,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    332\u001b[0m                                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfall_back_to_pt_during_load\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    333\u001b[0m                                        \u001b[38;5;28;01mTrue\u001b[39;00m)), )\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, module \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mnamed_modules():\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py:154\u001b[0m, in \u001b[0;36m_initialize_model\u001b[0;34m(model_config, load_config, lora_config, multimodal_config, cache_config, scheduler_config)\u001b[0m\n\u001b[1;32m    151\u001b[0m model_class \u001b[38;5;241m=\u001b[39m get_model_architecture(model_config)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    152\u001b[0m quant_config \u001b[38;5;241m=\u001b[39m _get_quantization_config(model_config, load_config)\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhf_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m                   \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_get_model_initialization_kwargs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultimodal_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mscheduler_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:340\u001b[0m, in \u001b[0;36mQwen2ForCausalLM.__init__\u001b[0;34m(self, config, cache_config, quant_config, lora_config)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlora_config \u001b[38;5;241m=\u001b[39m lora_config\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_config \u001b[38;5;241m=\u001b[39m quant_config\n\u001b[0;32m--> 340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2Model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mtie_word_embeddings:\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39membed_tokens\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:242\u001b[0m, in \u001b[0;36mQwen2Model.__init__\u001b[0;34m(self, config, cache_config, quant_config, prefix)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    239\u001b[0m     config\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    240\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    241\u001b[0m )\n\u001b[0;32m--> 242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m \u001b[43mmake_layers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_hidden_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mQwen2DecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.layers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py:146\u001b[0m, in \u001b[0;36mmake_layers\u001b[0;34m(num_hidden_layers, layer_fn, prefix)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_pp_indices\n\u001b[1;32m    142\u001b[0m start_layer, end_layer \u001b[38;5;241m=\u001b[39m get_pp_indices(num_hidden_layers,\n\u001b[1;32m    143\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mrank_in_group,\n\u001b[1;32m    144\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mworld_size)\n\u001b[1;32m    145\u001b[0m modules \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 146\u001b[0m     [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer)] \u001b[38;5;241m+\u001b[39m [\n\u001b[1;32m    147\u001b[0m         maybe_offload_to_cpu(layer_fn(prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer)\n\u001b[1;32m    149\u001b[0m     ] \u001b[38;5;241m+\u001b[39m [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_layer, num_hidden_layers)])\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m start_layer, end_layer, modules\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/utils.py:147\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mvllm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_pp_indices\n\u001b[1;32m    142\u001b[0m start_layer, end_layer \u001b[38;5;241m=\u001b[39m get_pp_indices(num_hidden_layers,\n\u001b[1;32m    143\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mrank_in_group,\n\u001b[1;32m    144\u001b[0m                                         get_pp_group()\u001b[38;5;241m.\u001b[39mworld_size)\n\u001b[1;32m    145\u001b[0m modules \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    146\u001b[0m     [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer)] \u001b[38;5;241m+\u001b[39m [\n\u001b[0;32m--> 147\u001b[0m         maybe_offload_to_cpu(\u001b[43mlayer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mprefix\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_layer, end_layer)\n\u001b[1;32m    149\u001b[0m     ] \u001b[38;5;241m+\u001b[39m [PPMissingLayer() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(end_layer, num_hidden_layers)])\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m start_layer, end_layer, modules\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:244\u001b[0m, in \u001b[0;36mQwen2Model.__init__.<locals>.<lambda>\u001b[0;34m(prefix)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_tokens \u001b[38;5;241m=\u001b[39m VocabParallelEmbedding(\n\u001b[1;32m    239\u001b[0m     config\u001b[38;5;241m.\u001b[39mvocab_size,\n\u001b[1;32m    240\u001b[0m     config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    241\u001b[0m )\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_layer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m make_layers(\n\u001b[1;32m    243\u001b[0m     config\u001b[38;5;241m.\u001b[39mnum_hidden_layers,\n\u001b[0;32m--> 244\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m prefix: \u001b[43mQwen2DecoderLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mcache_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    247\u001b[0m     prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.layers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    248\u001b[0m )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size, eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:184\u001b[0m, in \u001b[0;36mQwen2DecoderLayer.__init__\u001b[0;34m(self, config, cache_config, quant_config)\u001b[0m\n\u001b[1;32m    174\u001b[0m rope_scaling \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrope_scaling\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn \u001b[38;5;241m=\u001b[39m Qwen2Attention(\n\u001b[1;32m    176\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    177\u001b[0m     num_heads\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_attention_heads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    182\u001b[0m     quant_config\u001b[38;5;241m=\u001b[39mquant_config,\n\u001b[1;32m    183\u001b[0m     rope_scaling\u001b[38;5;241m=\u001b[39mrope_scaling)\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mQwen2MLP\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_act\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhidden_act\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    191\u001b[0m                                eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_layernorm \u001b[38;5;241m=\u001b[39m RMSNorm(config\u001b[38;5;241m.\u001b[39mhidden_size,\n\u001b[1;32m    193\u001b[0m                                         eps\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mrms_norm_eps)\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py:65\u001b[0m, in \u001b[0;36mQwen2MLP.__init__\u001b[0;34m(self, hidden_size, intermediate_size, hidden_act, quant_config)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     59\u001b[0m     hidden_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m     quant_config: Optional[QuantizationConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     63\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgate_up_proj \u001b[38;5;241m=\u001b[39m \u001b[43mMergedColumnParallelLinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mintermediate_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_proj \u001b[38;5;241m=\u001b[39m RowParallelLinear(intermediate_size,\n\u001b[1;32m     70\u001b[0m                                        hidden_size,\n\u001b[1;32m     71\u001b[0m                                        bias\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     72\u001b[0m                                        quant_config\u001b[38;5;241m=\u001b[39mquant_config)\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hidden_act \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msilu\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:387\u001b[0m, in \u001b[0;36mMergedColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_sizes, bias, gather_output, skip_bias_add, params_dtype, quant_config, prefix)\u001b[0m\n\u001b[1;32m    385\u001b[0m tp_size \u001b[38;5;241m=\u001b[39m get_tensor_model_parallel_world_size()\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mall\u001b[39m(output_size \u001b[38;5;241m%\u001b[39m tp_size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m output_size \u001b[38;5;129;01min\u001b[39;00m output_sizes)\n\u001b[0;32m--> 387\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m                 \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mgather_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgather_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mskip_bias_add\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_bias_add\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mquant_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquant_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:291\u001b[0m, in \u001b[0;36mColumnParallelLinear.__init__\u001b[0;34m(self, input_size, output_size, bias, gather_output, skip_bias_add, params_dtype, quant_config, output_sizes, prefix)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     output_sizes \u001b[38;5;241m=\u001b[39m [output_size]\n\u001b[0;32m--> 291\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquant_method\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bias:\n\u001b[1;32m    301\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;241m=\u001b[39m Parameter(\n\u001b[1;32m    302\u001b[0m         torch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_size_per_partition,\n\u001b[1;32m    303\u001b[0m                     dtype\u001b[38;5;241m=\u001b[39mparams_dtype))\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/linear.py:109\u001b[0m, in \u001b[0;36mUnquantizedLinearMethod.create_weights\u001b[0;34m(self, layer, input_size_per_partition, output_partition_sizes, input_size, output_size, params_dtype, **extra_weight_attrs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, layer: torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule,\n\u001b[1;32m    105\u001b[0m                    input_size_per_partition: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    106\u001b[0m                    output_partition_sizes: List[\u001b[38;5;28mint\u001b[39m], input_size: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    107\u001b[0m                    output_size: \u001b[38;5;28mint\u001b[39m, params_dtype: torch\u001b[38;5;241m.\u001b[39mdtype,\n\u001b[1;32m    108\u001b[0m                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_weight_attrs):\n\u001b[0;32m--> 109\u001b[0m     weight \u001b[38;5;241m=\u001b[39m Parameter(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_partition_sizes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43minput_size_per_partition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams_dtype\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    112\u001b[0m                        requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    113\u001b[0m     set_weight_attrs(weight, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_dim\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0\u001b[39m})\n\u001b[1;32m    114\u001b[0m     layer\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, weight)\n",
      "File \u001b[0;32m/scratch/jiarui14/miniconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_device.py:79\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 79\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 260.00 MiB. GPU 0 has a total capacity of 47.53 GiB of which 197.81 MiB is free. Including non-PyTorch memory, this process has 5.31 GiB memory in use. Process 2940433 has 42.00 GiB memory in use. Of the allocated memory 5.00 GiB is allocated by PyTorch, and 15.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# prepare model\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '8'\n",
    "llm = LLM(script_args.model_name_or_path, gpu_memory_utilization=0.5, dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_1_sampling():\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=script_args.max_length,\n",
    "        temperature=1.0,\n",
    "        n=script_args.stage_1_samples,\n",
    "    )\n",
    "    prompts = []\n",
    "    for i, item in enumerate(ds):\n",
    "        conv = [{'role': 'user', 'content': item['problem'] + f' Let\\'s think step by step and output the final answer within \\\\boxed{{}}'}]\n",
    "        conv_chat = tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n",
    "        prompts.append(conv_chat)\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2/2 [01:04<00:00, 32.39s/it, est. speed input: 3.75 toks/s, output: 158.34 toks/s]\n"
     ]
    }
   ],
   "source": [
    "outputs = stage_1_sampling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_ds = load_dataset('FlippyDora/math500_Qwen2-7B-Instruct_n8')['train'].select(range(script_args.start, script_args.end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [0, 1, 2, 3, 5, 6, 7], [0, 1, 2, 3, 4, 5, 6, 7], [0, 1, 2, 3, 4, 5, 6, 7], [0, 1, 2, 3, 4, 5, 6, 7]]\n"
     ]
    }
   ],
   "source": [
    "#TODO: currently, stage 1 selects all outputs with correct answers\n",
    "stage_1_collected_data = []\n",
    "corrects = []\n",
    "for i, item in enumerate(ds):\n",
    "    collected_data = {\n",
    "        'problem': item['problem'],\n",
    "        'answer': item['answer'],\n",
    "        'outputs': []\n",
    "    }\n",
    "    problem_corrects = []\n",
    "    for j in range(script_args.stage_1_samples):\n",
    "        # correct = reward_labeling.is_equal(outputs[i].outputs[j].text, item['answer'], dataset_name='math500')\n",
    "        correct = reward_labeling.is_equal(tmp_ds[i]['outputs'][j]['output'], item['answer'], dataset_name='math500')\n",
    "        if correct:\n",
    "            problem_corrects.append(j)\n",
    "            # collected_data['outputs'].append(outputs[i].outputs[j].text)\n",
    "            collected_data['outputs'].append(tmp_ds[i]['outputs'][j]['output'])\n",
    "    corrects.append(problem_corrects)\n",
    "    stage_1_collected_data.append(collected_data)\n",
    "\n",
    "print(corrects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accept rate from stage 1\n",
    "\n",
    "def calc_accept_rate():\n",
    "    accept_rates = []\n",
    "    for item in stage_1_collected_data:\n",
    "        accept_rate = len(item['outputs']) / script_args.stage_1_samples\n",
    "        accept_rates.append(accept_rate)\n",
    "    return accept_rates\n",
    "\n",
    "def calc_sample_ratio(Gs, ps):\n",
    "    # Gs: list of gradients\n",
    "    # ps: list of accept rates\n",
    "    sample_sizes = []\n",
    "    for G, p in zip(Gs, ps):\n",
    "        if G == 0 or p == 0:\n",
    "            sample_size = 0\n",
    "        else:\n",
    "            sample_size = G / (np.sqrt(p + script_args.alpha / np.power(p, script_args.beta - 1.0)))    \n",
    "        sample_sizes.append(sample_size)\n",
    "    total = sum(sample_sizes)\n",
    "    sample_sizes = [sample_size / total for sample_size in sample_sizes]\n",
    "    return sample_sizes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  5.21it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(152064, 3584)\n",
       "    (layers): ModuleList(\n",
       "      (0-27): 28 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=3584, bias=True)\n",
       "          (k_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (v_proj): Linear(in_features=3584, out_features=512, bias=True)\n",
       "          (o_proj): Linear(in_features=3584, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=18944, bias=False)\n",
       "          (down_proj): Linear(in_features=18944, out_features=3584, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model for gradient calculation\n",
    "model = AutoModelForCausalLM.from_pretrained(script_args.model_name_or_path, torch_dtype=torch.bfloat16)\n",
    "model.to(torch.device('cuda:8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_1_collected_data = [\n",
    "    {},\n",
    "    {'problem': ds[0]['problem'],\n",
    "     'answer': ds[0]['answer'],\n",
    "     'outputs': ['The answer is 1.']}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = [{'role': 'user', 'content': ds[1]['problem'] + f' Let\\'s think step by step and output the final answer within \\\\boxed{{}}'}]\n",
    "#  {'role': 'assistant', 'content': stage_1_collected_data[1]['outputs'][0]}]\n",
    "conv_chat = tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n",
    "conv_chat += stage_1_collected_data[1]['outputs'][0]\n",
    "input_ids = tokenizer(conv_chat, return_tensors='pt').input_ids.to(torch.device('cuda:8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_prompt_end(input_ids):\n",
    "    end = tokenizer('<|im_start|>assistant\\n')['input_ids']\n",
    "    end_len = len(end)\n",
    "    input_len = len(input_ids)\n",
    "    for i in range(input_len - end_len):\n",
    "        found = True\n",
    "        for j in range(end_len):\n",
    "            if input_ids[i + j] != end[j]:\n",
    "                found = False\n",
    "                break\n",
    "        if found:\n",
    "            return i + end_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad():\n",
    "    all_grads = []\n",
    "    for i, item in enumerate(tqdm(stage_1_collected_data, desc='Calculating gradients')):\n",
    "        if len(item['outputs']) == 0:\n",
    "            mean_grad = 0\n",
    "        else:\n",
    "            grads = []\n",
    "            for output in item['outputs']:\n",
    "                conv = [\n",
    "                    {'role': 'system', 'content': 'Please reason step by step, and put your final answer within \\\\boxed{{}}.'},\n",
    "                    {'role': 'user', 'content': item['problem'] + f' Let\\'s think step by step and output the final answer within \\\\boxed{{}}'}\n",
    "                ]\n",
    "                conv_chat = tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n",
    "                conv_chat += output\n",
    "                input_ids = tokenizer(conv_chat, return_tensors='pt').input_ids.to(model.device)\n",
    "                o = model(input_ids, output_hidden_states=True)\n",
    "                logits = o.logits\n",
    "                log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "                resp_start = find_prompt_end(input_ids[0].tolist())\n",
    "                output_log_probs = log_probs[0, resp_start:]\n",
    "                output_log_probs_sen = output_log_probs.sum(dim=0)\n",
    "                \n",
    "                # get the gradient by loss backpropagation\n",
    "                loss = -output_log_probs_sen.mean() / (len(input_ids[0]) - resp_start)\n",
    "                loss.backward()\n",
    "                grad_norm = torch.norm(model.lm_head.weight.grad, p=2).item()\n",
    "                grads.append(grad_norm)\n",
    "                model.zero_grad()\n",
    "\n",
    "            mean_grad = np.mean(grads)\n",
    "        all_grads.append(mean_grad)\n",
    "\n",
    "    return all_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating gradients: 100%|██████████| 5/5 [00:13<00:00,  2.77s/it]\n"
     ]
    }
   ],
   "source": [
    "all_grads = calc_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "accept_rates = calc_accept_rate()\n",
    "sample_sizes = calc_sample_ratio(all_grads, accept_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 2 2 2]\n",
      "[0, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "def float_to_int_preserve_sum(arr, N):\n",
    "    # 1. 初步缩放并四舍五入\n",
    "    scaled_arr = np.array(arr) * N\n",
    "    int_arr = np.round(scaled_arr).astype(int)\n",
    "    print(int_arr)\n",
    "\n",
    "    # 2. 计算误差\n",
    "    error = N - np.sum(int_arr)\n",
    "\n",
    "    # 3. 误差修正：根据四舍五入前的误差最小调整\n",
    "    if error != 0:\n",
    "        # 计算原始浮点数和转换后整数的误差\n",
    "        residuals = scaled_arr - int_arr\n",
    "        # 按误差绝对值最大调整\n",
    "        indices = np.argsort(-residuals if error > 0 else residuals)[:abs(error)]\n",
    "        int_arr[indices] += np.sign(error)  # 调整以匹配总和\n",
    "\n",
    "    return int_arr.tolist()\n",
    "\n",
    "sample_sizes = float_to_int_preserve_sum(sample_sizes, script_args.stage_2_samples)\n",
    "print(sample_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stage_2_sampling(sample_sizes):\n",
    "    sampling_params = SamplingParams(\n",
    "        max_tokens=script_args.max_length,\n",
    "        temperature=1.0,\n",
    "        n=1,\n",
    "    )\n",
    "    prompts = []\n",
    "    for i, item in enumerate(ds):\n",
    "        conv = [{'role': 'user', 'content': item['problem'] + f' Let\\'s think step by step and output the final answer within \\\\boxed{{}}'}]\n",
    "        conv_chat = tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n",
    "        for _ in sample_sizes[i]:\n",
    "            prompts.append(conv_chat)\n",
    "    outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "    idx_sum = 0\n",
    "    new_outputs = []\n",
    "    for i in range(len(sample_sizes)):\n",
    "        new_outputs.append([])\n",
    "        for idx in range(idx_sum, idx_sum + sample_sizes[i]):\n",
    "            new_outputs[-1].append(outputs[idx].outputs[0].text)\n",
    "\n",
    "        idx_sum += sample_sizes[i]\n",
    "\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_2_outputs = stage_2_sampling(sample_sizes)\n",
    "stage_2_collected_data = []\n",
    "corrects_2 = []\n",
    "for i, item in enumerate(ds):\n",
    "    collected_data = {\n",
    "        'problem': item['problem'],\n",
    "        'answer': item['answer'],\n",
    "        'outputs': []\n",
    "    }\n",
    "    problem_corrects = []\n",
    "    for j in range(len(stage_2_outputs[i])):\n",
    "        # correct = reward_labeling.is_equal(outputs[i].outputs[j].text, item['answer'], dataset_name='math500')\n",
    "        correct = reward_labeling.is_equal(stage_2_outputs[i][j], item['answer'], dataset_name='math500')\n",
    "        if correct:\n",
    "            problem_corrects.append(j)\n",
    "            # collected_data['outputs'].append(outputs[i].outputs[j].text)\n",
    "            collected_data['outputs'].append(tmp_ds[i]['outputs'][j]['output'])\n",
    "    corrects_2.append(problem_corrects)\n",
    "    stage_2_collected_data.append(collected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 6639.56it/s]\n"
     ]
    }
   ],
   "source": [
    "ds = load_dataset('dsrtrain/numia_prompt')['train'].shuffle(seed=1).select(range(10000))\n",
    "problems = []\n",
    "for item in tqdm(ds):\n",
    "    if item['problem'] not in problems:\n",
    "        problems.append(item['problem'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data_source': ['numina_amc_aime',\n",
       "  'numina_synthetic_math',\n",
       "  'numina_synthetic_amc',\n",
       "  'numina_synthetic_math',\n",
       "  'numina_cn_k12'],\n",
       " 'prompt': [[{'content': '\\nWhen tackling complex reasoning tasks, you have access to the following actions. Use them as needed to progress through your thought process.\\n\\n[ASSESS]\\n\\n[ADVANCE]\\n\\n[VERIFY]\\n\\n[SIMPLIFY]\\n\\n[SYNTHESIZE]\\n\\n[PIVOT]\\n\\n[OUTPUT]\\n\\nYou should strictly follow the format below:\\n\\n[ACTION NAME]\\n\\n# Your action step 1\\n\\n# Your action step 2\\n\\n# Your action step 3\\n\\n...\\n\\nNext action: [NEXT ACTION NAME]\\n\\n',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Given the parabola $y = x^2 - 8x + c$, determine the value of $c$ for which the vertex of the parabola will be a point on the $x$-axis.\\n\\nPresent the answer in LaTex format: \\\\boxed{Your answer}',\n",
       "    'role': 'user'}],\n",
       "  [{'content': '\\nWhen tackling complex reasoning tasks, you have access to the following actions. Use them as needed to progress through your thought process.\\n\\n[ASSESS]\\n\\n[ADVANCE]\\n\\n[VERIFY]\\n\\n[SIMPLIFY]\\n\\n[SYNTHESIZE]\\n\\n[PIVOT]\\n\\n[OUTPUT]\\n\\nYou should strictly follow the format below:\\n\\n[ACTION NAME]\\n\\n# Your action step 1\\n\\n# Your action step 2\\n\\n# Your action step 3\\n\\n...\\n\\nNext action: [NEXT ACTION NAME]\\n\\n',\n",
       "    'role': 'system'},\n",
       "   {'content': 'In a similar setup, square $PQRS$ is constructed along diameter $PQ$ of a semicircle. The semicircle and square $PQRS$ are coplanar. Line segment $PQ$ has a length of 8 centimeters. If point $N$ is the midpoint of arc $PQ$, what is the length of segment $NS$?\\n\\nPresent the answer in LaTex format: \\\\boxed{Your answer}',\n",
       "    'role': 'user'}],\n",
       "  [{'content': '\\nWhen tackling complex reasoning tasks, you have access to the following actions. Use them as needed to progress through your thought process.\\n\\n[ASSESS]\\n\\n[ADVANCE]\\n\\n[VERIFY]\\n\\n[SIMPLIFY]\\n\\n[SYNTHESIZE]\\n\\n[PIVOT]\\n\\n[OUTPUT]\\n\\nYou should strictly follow the format below:\\n\\n[ACTION NAME]\\n\\n# Your action step 1\\n\\n# Your action step 2\\n\\n# Your action step 3\\n\\n...\\n\\nNext action: [NEXT ACTION NAME]\\n\\n',\n",
       "    'role': 'system'},\n",
       "   {'content': 'Two concentric circles share the same center C. Chord $\\\\overline{AD}$ is tangent to the inner circle at point B, where AC = 12 (radius of the outer circle), and the length of chord $\\\\overline{AD}$ is 20. Calculate the area between the two circles.\\n\\nPresent the answer in LaTex format: \\\\boxed{Your answer}',\n",
       "    'role': 'user'}],\n",
       "  [{'content': '\\nWhen tackling complex reasoning tasks, you have access to the following actions. Use them as needed to progress through your thought process.\\n\\n[ASSESS]\\n\\n[ADVANCE]\\n\\n[VERIFY]\\n\\n[SIMPLIFY]\\n\\n[SYNTHESIZE]\\n\\n[PIVOT]\\n\\n[OUTPUT]\\n\\nYou should strictly follow the format below:\\n\\n[ACTION NAME]\\n\\n# Your action step 1\\n\\n# Your action step 2\\n\\n# Your action step 3\\n\\n...\\n\\nNext action: [NEXT ACTION NAME]\\n\\n',\n",
       "    'role': 'system'},\n",
       "   {'content': 'What is the units digit of the sum of the squares of the first 2035 odd, positive integers?\\n\\nPresent the answer in LaTex format: \\\\boxed{Your answer}',\n",
       "    'role': 'user'}],\n",
       "  [{'content': '\\nWhen tackling complex reasoning tasks, you have access to the following actions. Use them as needed to progress through your thought process.\\n\\n[ASSESS]\\n\\n[ADVANCE]\\n\\n[VERIFY]\\n\\n[SIMPLIFY]\\n\\n[SYNTHESIZE]\\n\\n[PIVOT]\\n\\n[OUTPUT]\\n\\nYou should strictly follow the format below:\\n\\n[ACTION NAME]\\n\\n# Your action step 1\\n\\n# Your action step 2\\n\\n# Your action step 3\\n\\n...\\n\\nNext action: [NEXT ACTION NAME]\\n\\n',\n",
       "    'role': 'system'},\n",
       "   {'content': 'It is known that $O$ is the origin of coordinates, $\\\\overrightarrow{OA}=(2\\\\cos x, \\\\sqrt{3})$, $\\\\overrightarrow{OB}=(\\\\sin x + \\\\sqrt{3}\\\\cos x,-1)$, and if $f(x)=\\\\overrightarrow{OA} \\\\cdot \\\\overrightarrow{OB}+2$.\\nⅠ) Find the interval over which the function $f(x)$ is monotonically decreasing;\\nⅡ) When $x \\\\in \\\\left(0, \\\\frac{\\\\pi}{2}\\\\right)$, if the equation $f(x)+m=0$ has a root, find the range of values for $m$.\\n\\nPresent the answer in LaTex format: \\\\boxed{Your answer}',\n",
       "    'role': 'user'}]],\n",
       " 'ability': ['math', 'math', 'math', 'math', 'math'],\n",
       " 'reward_model': [{'ground_truth': '16', 'style': 'rule'},\n",
       "  {'ground_truth': '4\\\\sqrt{10}', 'style': 'rule'},\n",
       "  {'ground_truth': '100\\\\pi', 'style': 'rule'},\n",
       "  {'ground_truth': '5', 'style': 'rule'},\n",
       "  {'ground_truth': '[-4, \\\\sqrt{3}-2)', 'style': 'rule'}],\n",
       " 'extra_info': [{'index': 0, 'split': 'dummy'},\n",
       "  {'index': 0, 'split': 'dummy'},\n",
       "  {'index': 0, 'split': 'dummy'},\n",
       "  {'index': 0, 'split': 'dummy'},\n",
       "  {'index': 0, 'split': 'dummy'}],\n",
       " 'problem': ['Given the parabola $y = x^2 - 8x + c$, determine the value of $c$ for which the vertex of the parabola will be a point on the $x$-axis.',\n",
       "  'In a similar setup, square $PQRS$ is constructed along diameter $PQ$ of a semicircle. The semicircle and square $PQRS$ are coplanar. Line segment $PQ$ has a length of 8 centimeters. If point $N$ is the midpoint of arc $PQ$, what is the length of segment $NS$?',\n",
       "  'Two concentric circles share the same center C. Chord $\\\\overline{AD}$ is tangent to the inner circle at point B, where AC = 12 (radius of the outer circle), and the length of chord $\\\\overline{AD}$ is 20. Calculate the area between the two circles.',\n",
       "  'What is the units digit of the sum of the squares of the first 2035 odd, positive integers?',\n",
       "  'It is known that $O$ is the origin of coordinates, $\\\\overrightarrow{OA}=(2\\\\cos x, \\\\sqrt{3})$, $\\\\overrightarrow{OB}=(\\\\sin x + \\\\sqrt{3}\\\\cos x,-1)$, and if $f(x)=\\\\overrightarrow{OA} \\\\cdot \\\\overrightarrow{OB}+2$.\\nⅠ) Find the interval over which the function $f(x)$ is monotonically decreasing;\\nⅡ) When $x \\\\in \\\\left(0, \\\\frac{\\\\pi}{2}\\\\right)$, if the equation $f(x)+m=0$ has a root, find the range of values for $m$.']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = model(input_ids, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = o.logits\n",
    "log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "output_log_probs = log_probs[0, find_prompt_end(input_ids[0].tolist()):]\n",
    "output_log_probs_sen = output_log_probs.sum(dim=0) # whole sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -output_log_probs_sen.mean()\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampling_params = SamplingParams(\n",
    "#     temperature=1.0,\n",
    "#     n=8,\n",
    "#     max_tokens=script_args.max_length,\n",
    "#     logprobs=1,\n",
    "# )\n",
    "\n",
    "# # generate\n",
    "# conv = [{'role': 'user', 'content': ds[0]['problem'] + f' Let\\'s think step by step and output the final answer within \\\\boxed{{}}'}]\n",
    "# conv_chat = tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=True)\n",
    "# print(conv_chat)\n",
    "# prompts = [conv_chat]\n",
    "# outputs = llm.generate(prompts, sampling_params)\n",
    "\n",
    "# print(type(outputs))\n",
    "\n",
    "# def get_logprobs_vllm(prompts, sampling_params):\n",
    "#     outputs = llm.generate(prompts, sampling_params)\n",
    "#     logprobs = []\n",
    "#     for output in outputs:\n",
    "#         logprobs.append([])\n",
    "#         for item in output.outputs:\n",
    "#             logprobs[-1].append(item.cumulative_logprob)\n",
    "\n",
    "#     return logprobs\n",
    "\n",
    "# def get_uniform_rand(l, r):\n",
    "#     return np.random.uniform(l, r)\n",
    "\n",
    "# print('done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vllm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
